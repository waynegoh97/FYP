{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d487d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65c512ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv   \n",
    "fields=['first','second']\n",
    "fid = \"b0f0\"\n",
    "for i in range(10):\n",
    "    with open(r'{}.csv'.format(fid), 'a', newline = '') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"12324\",\"23fa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea06778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t in tensor:\n",
    "            t.mul_(self.std).add_(self.mean)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "    \n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self,img_path, label, transform):\n",
    "        self.transform=transform\n",
    "        #self.img_folder=img_folder\n",
    "        self.img_path = img_path\n",
    "        self.label = label\n",
    "\n",
    "#The __len__ function returns the number of samples in our dataset.\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    " \n",
    "    def __getitem__(self,index):\n",
    "     \n",
    "        image=Image.open(self.img_path[index],'r')\n",
    "        image=self.transform(image)\n",
    "        targets=self.label[index]\n",
    "\n",
    "        return image, targets\n",
    "    \n",
    "def label_directory(img_path):\n",
    "    '''\n",
    "Purpose: Get all labels directory in a list (e.g. C://path/images/b0f0/[latitude_longitude])\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_path : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    label_dir : list\n",
    "        labels directory\n",
    "\n",
    "    '''\n",
    "    label_dir = []\n",
    "    for root, dirs, files in os.walk(img_path, topdown=False):\n",
    "        for d in dirs:\n",
    "            label_dir.append(os.path.join(root, d))\n",
    "    return label_dir\n",
    "\n",
    "def data_and_label(img_folder):\n",
    "    '''\n",
    "    Purpose: maps image path, image name, and labels together in list order\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_folder : string\n",
    "        directory of images (e.g. ./cnn_images/b0f0_train_with_labels)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    img_path : list\n",
    "        contains image paths of all images in the folder\n",
    "    label : array\n",
    "        contains array of all labels\n",
    "\n",
    "    '''\n",
    "    img_path = []\n",
    "    label = []\n",
    "    img_name = []\n",
    "    for root, dirs, files in os.walk(img_folder, topdown=False):\n",
    "        for name in files:\n",
    "            temp = os.path.join(root, name)\n",
    "            temp = os.path.normpath(temp)\n",
    "            path = temp.split(os.sep)\n",
    "            path = path[-2].split(\"_\")\n",
    "            path[0] = float(path[0])\n",
    "            path[1] = float(path[1])\n",
    "            label.append(path)\n",
    "            img_path.append(temp)\n",
    "            img_name.append(name)\n",
    "    label = np.array(label)\n",
    "    return img_path, label, img_name\n",
    "\n",
    "def norm_image(path, label):\n",
    "    '''\n",
    "\n",
    "    Parameters\n",
    "    ----------       \n",
    "    path : string\n",
    "        directory of each image\n",
    "    label : array\n",
    "        scaled labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean : float\n",
    "        mean value of all the images\n",
    "    std : TYPE\n",
    "        standard deviation of all the images\n",
    "\n",
    "    '''\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    data_set = ImageDataset(path, label, transform)\n",
    "    loader = DataLoader(data_set, batch_size=len(data_set))\n",
    "    data = next(iter(loader))\n",
    "    mean = data[0].mean()\n",
    "    std = data[0].std()\n",
    "    return mean, std\n",
    "\n",
    "def load_image(img_path, label, batch_size, mean, std, shuffle):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_path : string\n",
    "        directory of dataset used\n",
    "    batch_size : int\n",
    "        batch size\n",
    "    mean : float\n",
    "        mean of overall input data\n",
    "    std : float\n",
    "        standard deviation of overall input data\n",
    "    shuffle : boolean\n",
    "        shuffle dataloader or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataloader\n",
    "\n",
    "    '''\n",
    "\n",
    "    transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                         transforms.ToTensor(),transforms.Normalize((mean),(std))])\n",
    "    # , transforms.Normalize((mean),(std))\n",
    "    data_set=ImageDataset(img_path, label,transform)\n",
    "    dataloader = DataLoader(data_set,batch_size=batch_size,shuffle=shuffle, drop_last = True)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "def load_by_label(label_dir, batch_size, shuffle=True):\n",
    "    '''\n",
    "Purpose: Get dataloader by individual label\n",
    "    Parameters\n",
    "    ----------\n",
    "    label_dir : TYPE\n",
    "        DESCRIPTION.\n",
    "    batch_size : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataloader : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    img_path, label, img_name = data_and_label(label_dir)\n",
    "    mean, std = norm_image(img_path, label)\n",
    "    dataloader = load_image(img_path, label, batch_size, mean, std, shuffle)\n",
    "    return dataloader, mean, std, img_name, label\n",
    "\n",
    "# For 23x23 images\n",
    "class UJI_Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(UJI_Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # input: N x channels_img x 23 x 23\n",
    "            nn.Conv2d(\n",
    "                channels_img, features_d*2, kernel_size=3, stride=2, padding=1\n",
    "            ), #64x12x12\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._block(features_d*2, features_d * 4, 4, 2, 1), #128x6x6\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1), #256x3x3\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=3, stride=1, padding=0), # 1x1x1\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "# Generator: noise > decreasing node + batchnorm + relu > feature critic layer (64) \n",
    "class UJI_Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(UJI_Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._block(channels_noise, features_g * 8, 3, 1, 0),  # img: 3x3\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 6x6\n",
    "            self._block(features_g * 4, features_g*2 , 4, 2, 1),  # img: 12x12\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g*2, channels_img, kernel_size=3, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 23x23\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "def initialize_weights(model):\n",
    "    # Initializes weights for wgan-gp\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "            \n",
    "def gradient_penalty(critic, real, fake, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty\n",
    "\n",
    "def wgan_gp_pretrain(save_state, gen_model_state, disc_model_state, data_dir, \n",
    "                     lr, batch, num_epoch, img_dim, img_channel, latent, critic_layer, gen_layer, critic_iter, gradient_p):\n",
    "    '''\n",
    "    Purpose: Training of WGAN-GP model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_state_dir : string\n",
    "        model state name to be saved as\n",
    "    data_dir : string\n",
    "        folder name with the training input images\n",
    "    dataset : string\n",
    "        define the type of dataset used (e.g. uji or ng)\n",
    "    lr : float\n",
    "        learning rate\n",
    "    batch : int\n",
    "        batch size\n",
    "    num_epoch : int\n",
    "        number of epochs\n",
    "    img_dim : int\n",
    "        image size\n",
    "    img_channel : int\n",
    "        number of channels (e.g. 1 for grayscale, 3 for rgb)\n",
    "    latent : int\n",
    "        latent noise\n",
    "    critic_layer : int\n",
    "        determine number of discriminator layer\n",
    "    gen_layer : int\n",
    "        determine number of generator layer\n",
    "    critic_iter : int\n",
    "        determine number of critic iterations\n",
    "    gradient_p : int\n",
    "        gradient penalty\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dataloader, mean, std, _, _= load_by_label(data_dir, batch)\n",
    "    unorm = UnNormalize(mean = mean, std = std)\n",
    "        \n",
    "    gen = UJI_Generator(latent, img_channel, gen_layer).to(device)\n",
    "    critic = UJI_Discriminator(img_channel, critic_layer).to(device)\n",
    "    gen.load_state_dict(torch.load(gen_model_state))\n",
    "    critic.load_state_dict(torch.load(disc_model_state))\n",
    "    \n",
    "        \n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.0,0.9))\n",
    "    opt_critic = optim.Adam(critic.parameters(), lr=lr, betas=(0.0,0.9))\n",
    "    gen.train()\n",
    "    critic.train()\n",
    "    critic_loss = []\n",
    "\n",
    "    print(\"Starting\")\n",
    "    for epoch in range(num_epoch):\n",
    "        # Target labels not needed\n",
    "        gen.train()\n",
    "        for batch_idx, (data, _) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            cur_batch_size = data.shape[0]\n",
    "    \n",
    "            # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
    "            for _ in range(critic_iter):\n",
    "                noise = torch.randn(cur_batch_size, latent, 1, 1).to(device)\n",
    "                fake = gen(noise)\n",
    "                critic_real = critic(data).reshape(-1)\n",
    "                critic_fake = critic(fake).reshape(-1)\n",
    "                gp = gradient_penalty(critic, data, fake, device=device)\n",
    "                loss_critic = (-(torch.mean(critic_real) - torch.mean(critic_fake)) + gradient_p * gp)\n",
    "                critic.zero_grad()\n",
    "                loss_critic.backward(retain_graph=True)\n",
    "                opt_critic.step()\n",
    "    \n",
    "    \n",
    "            # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "            gen_fake = critic(fake).reshape(-1)\n",
    "            loss_gen = -torch.mean(gen_fake)\n",
    "            gen.zero_grad()\n",
    "            loss_gen.backward()\n",
    "            opt_gen.step()\n",
    "         \n",
    "  \n",
    "            \n",
    "        # print(\n",
    "        #     \"[Epoch: %d/%d] [Batch: %d/%d] [G loss: %f] [C loss: %f]\"\n",
    "        #     % (epoch+1, num_epoch, batch_idx+1, len(dataloader), loss_gen.detach(), loss_critic.detach())\n",
    "        # )\n",
    "        critic_loss.append(-loss_critic.detach())\n",
    "                        \n",
    "    torch.save(gen.state_dict(), save_state)\n",
    "    print(\"Ending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d09ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MIOpen(HIP): Warning [SQLiteBase] Unable to read system database file:/opt/rocm/miopen/share/miopen/db/gfx906_60.kdb Performance may degrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending\n",
      "['-7404.491683006287_4864890.416642644', 451.5463619492948]\n",
      "Starting\n",
      "Ending\n",
      "['-7410.026599999517_4864887.749600001', 442.18666095100343]\n",
      "Starting\n",
      "Ending\n",
      "['-7410.692299999297_4864890.038199998', 449.40419285185635]\n",
      "Starting\n",
      "Ending\n",
      "['-7411.32860000059_4864885.4059000015', 443.7083158576861]\n",
      "Starting\n",
      "Ending\n",
      "['-7417.831551343203_4864865.352306284', 451.92941021267325]\n",
      "Starting\n",
      "Ending\n",
      "['-7419.789388634264_4864861.91686368', 448.9426442049444]\n",
      "Starting\n",
      "Ending\n",
      "['-7421.1453999988735_4864865.389200002', 446.7122893529013]\n",
      "Starting\n",
      "Ending\n",
      "['-7421.30021238327_4864859.197052851', 446.6406229753047]\n",
      "Starting\n",
      "Ending\n",
      "['-7421.933200001717_4864863.970899999', 446.96744654048234]\n",
      "Starting\n",
      "Ending\n",
      "['-7422.258999999613_4864895.579300001', 446.22813763003796]\n",
      "Starting\n",
      "Ending\n",
      "['-7423.060899998993_4864877.686099999', 452.12018348183483]\n",
      "Starting\n",
      "Ending\n",
      "['-7423.182180404663_4864855.7189594135', 450.47270187549293]\n",
      "Starting\n",
      "Ending\n",
      "['-7424.301600001752_4864859.706600003', 452.0021704379469]\n",
      "Starting\n",
      "Ending\n",
      "['-7424.407800000161_4864901.759999998', 452.2743892893195]\n",
      "Starting\n",
      "Ending\n",
      "['-7424.804142381996_4864898.435409412', 444.13880176190287]\n",
      "Starting\n",
      "Ending\n",
      "['-7425.090500000864_4864858.286499999', 446.5745964292437]\n",
      "Starting\n",
      "Ending\n",
      "['-7425.661100000143_4864873.132700004', 443.03671429492533]\n",
      "Starting\n",
      "Ending\n",
      "['-7425.728100001812_4864901.3354', 449.1094954870641]\n",
      "Starting\n",
      "Ending\n",
      "['-7426.483600001782_4864899.974799998', 442.2762079630047]\n",
      "Starting\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE =0.001 #0.001 (mnist)\n",
    "BATCH_SIZE = 4 #32 (mnist), 8 for wgan-gp uji\n",
    "IMAGE_SIZE = 23\n",
    "CHANNELS_IMG = 1\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 1000\n",
    "FEATURES_CRITIC = 64\n",
    "FEATURES_GEN = 64\n",
    "CRITIC_ITERATIONS = 5\n",
    "LAMBDA_GP = 10\n",
    "my_dpi = 96 # Can be found using this link https://www.infobyip.com/detectmonitordpi.php\n",
    "\n",
    "gen_saved_state = '/home/SEANGLIDET/CZ1016/FYP_data/model_states/gen_most_sample.pt'\n",
    "disc_saved_state = '/home/SEANGLIDET/CZ1016/FYP_data/model_states/disc_most_sample.pt'\n",
    "\n",
    "unique_loc = \"b1f0\" \n",
    "\n",
    "data_dir = \"/home/SEANGLIDET/CZ1016/FYP_data/images/uji/ori_dirich/\"+unique_loc\n",
    "saved_state = \"/home/SEANGLIDET/CZ1016/FYP_data/model_states/UJI_fine_tuning/\"+unique_loc\n",
    "\n",
    "if not os.path.exists(saved_state):\n",
    "    os.makedirs(saved_state)\n",
    "NUM_EPOCHS = 500\n",
    "label_dir = label_directory(data_dir)\n",
    "\n",
    "time_keeper = []\n",
    "for i in range(len(label_dir)):\n",
    "    start = time.perf_counter()\n",
    "    curr_label = label_dir[i].split('/') #for linux \n",
    "    save_state = saved_state + '/'+str(curr_label[-1])+'.pt'\n",
    "\n",
    "    wgan_gp_pretrain(save_state, gen_saved_state, disc_saved_state, label_dir[i], LEARNING_RATE, BATCH_SIZE, NUM_EPOCHS, IMAGE_SIZE, \n",
    "                      CHANNELS_IMG, Z_DIM, FEATURES_CRITIC, FEATURES_GEN, CRITIC_ITERATIONS, LAMBDA_GP)\n",
    "    end = time.perf_counter()\n",
    "    print([curr_label[-1], end-start])\n",
    "    time_keeper.append([curr_label[-1], end-start])\n",
    "    \n",
    "df = pd.DataFrame(time_keeper, columns = [\"Labels\", \"Execution\"])\n",
    "df.to_csv(\"/home/SEANGLIDET/CZ1016/FYP_data/model_states/UJI_fine_tuning/runtime/\"+unique_loc+\"_runtime.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26707852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
